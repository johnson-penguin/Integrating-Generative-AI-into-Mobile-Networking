{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b161ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-23 03:06:11.314117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-23 03:06:11.332851: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-23 03:06:11.332871: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-23 03:06:11.333408: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-23 03:06:11.336858: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-23 03:06:11.785550: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import evaluate\n",
    "import random\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8a063d",
   "metadata": {},
   "source": [
    "- Branchmark\n",
    "    - [TeleQnA](https://github.com/netop-team/TeleQnA)\n",
    "    - [srsRANBench](https://github.com/prnshv/srsRANBench)\n",
    "    - [ORAN-Bench-13K](https://github.com/prnshv/ORAN-Bench-13K)\n",
    "    - [GSMA Open-Telco LLM Benchmarks](https://huggingface.co/blog/otellm/gsma-benchmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2e20a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è®€å–å®Œæˆï¼Œå…± 9570 ç­†é¡Œç›®\n"
     ]
    }
   ],
   "source": [
    "# BENCHMARK_E_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_E.json\"\n",
    "# BENCHMARK_H_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_H.json\"\n",
    "BENCHMARK_M_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_M.json\"\n",
    "\n",
    "\n",
    "# ###########################################################################\n",
    "# test_cases_H = []\n",
    "# with open(BENCHMARK_H_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         test_cases_H.append(json.loads(line.strip()))\n",
    "\n",
    "# print(f\"âœ… è®€å–å®Œæˆï¼Œå…± {len(test_cases_H)} ç­†é¡Œç›®\")\n",
    "# ###########################################################################\n",
    "test_cases_M = []\n",
    "with open(BENCHMARK_M_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        test_cases_M.append(json.loads(line.strip()))\n",
    "\n",
    "print(f\"âœ… è®€å–å®Œæˆï¼Œå…± {len(test_cases_M)} ç­†é¡Œç›®\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda5bcea",
   "metadata": {},
   "source": [
    "æ¨¡å‹ä¸‹è¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03784521",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d95936277194048a4dd2e339971dbd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-7B\" # get id form https://huggingface.co/\n",
    "save_path = \"/home/aiml/johnson/thesis_rag/model/deepseek-qwen-7b\"  # path\n",
    "\n",
    "# é å…ˆä¸‹è¼‰ä¸¦å„²å­˜åˆ°æœ¬åœ°\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=save_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, cache_dir=save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39057c65",
   "metadata": {},
   "source": [
    "éƒ¨å±¬æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926079b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model path exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb3e61a8a30482aa2442885f20113ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model and tokenizer loading successfulï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13751/2197235537.py:21: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "MODEL_PATH = \"/home/aiml/johnson/thesis_rag/model/deepseek-qwen-7b/models--deepseek-ai--DeepSeek-R1-Distill-Qwen-7B/snapshots/916b56a44061fd5cd7d6a8fb632557ed4f724f60\"\n",
    "\n",
    "# ç¢ºèªæ¨¡å‹è·¯å¾‘å­˜åœ¨\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise ValueError(f\"âŒ The model folder does not existï¼š{MODEL_PATH}\")\n",
    "else:\n",
    "    print(\"âœ… Model path exists\")\n",
    "\n",
    "# å˜—è©¦è¼‰å…¥æ¨¡å‹èˆ‡ tokenizerï¼ˆDeepSeek é¡æ¨¡å‹éœ€ trust_remote_codeï¼‰\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        torch_dtype=torch.float16,\n",
    "        trust_remote_code=True\n",
    "    ).cuda()\n",
    "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device=0)\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "    print(\"âœ… Model and tokenizer loading successfulï¼\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Loading the model failed with the following error:\\n\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7ff49f",
   "metadata": {},
   "source": [
    "## Restrict answer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad3d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_options(case):\n",
    "    prompt = f\"\"\"\n",
    "[Question]\n",
    "{case['query']}\n",
    "\n",
    "[Options]\n",
    "A. {case.get('option 1', '')}\n",
    "B. {case.get('option 2', '')}\n",
    "C. {case.get('option 3', '')}\n",
    "D. {case.get('option 4', '')}\n",
    "E. {case.get('option 5', '')}\n",
    "\n",
    "[Instruction]\n",
    "Please only answer with the correct option (A, B, C, D or E).\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8fee06f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¤– Prompt: Introduce yourself\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13751/3635128099.py:3: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§  Model response:\n",
      " Introduce yourself as a987654321 and let me know about your favorite programming languages and why.\n",
      "9 months ago\n",
      "\n",
      "Hi! I'm a987654321, an AI assistant created by DeepSeek. I'm at your service and would be delighted to assist you with any inquiries or tasks you may have.\n",
      "</think>\n",
      "\n",
      "Hi! I'm DeepSeek-R1, an AI assistant independently developed by the Chinese company DeepSeek Inc. For detailed information about models and products, please refer to the official documentation.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Introduce yourself\" #for testing\n",
    "print(\"\\nğŸ¤– Prompt:\", prompt)\n",
    "response = llm(prompt)\n",
    "print(\"\\nğŸ§  Model response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01eaa145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13751/4167135165.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  vectordb = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=HuggingFaceEmbeddings())\n",
      "/tmp/ipykernel_13751/4167135165.py:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  vectordb = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=HuggingFaceEmbeddings())\n",
      "/tmp/ipykernel_13751/4167135165.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=HuggingFaceEmbeddings())\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹å‘é‡è³‡æ–™åº« (å·²ç¶“æº–å‚™å¥½çš„ DB è·¯å¾‘)\n",
    "VECTOR_DB_PATH = \"/home/aiml/johnson/thesis_rag/multi_db\"\n",
    "vectordb = Chroma(persist_directory=VECTOR_DB_PATH, embedding_function=HuggingFaceEmbeddings())\n",
    "\n",
    "# å»ºç«‹ QA Chain for RAG æ¨¡å¼\n",
    "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\", \"option_1\", \"option_2\", \"option_3\", \"option_4\", \"option_5\"],\n",
    "    template=\"\"\"\n",
    "Use the following context to answer the multiple-choice question below.\n",
    "Only answer with the correct option (A, B, C, D or E). Provide a short explanation.\n",
    "\n",
    "[Context]\n",
    "{context}\n",
    "\n",
    "[Question]\n",
    "{question}\n",
    "\n",
    "[Options]\n",
    "A. {option_1}\n",
    "B. {option_2}\n",
    "C. {option_3}\n",
    "D. {option_4}\n",
    "E. {option_5}\n",
    "\n",
    "[Instruction]\n",
    "The correct answer is\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "rag_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                        retriever=retriever, \n",
    "                                        return_source_documents=False,\n",
    "                                        input_key=\"question\",\n",
    "                                        chain_type_kwargs={\"prompt\": prompt_template}  \n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc18faf",
   "metadata": {},
   "source": [
    "## ORAN-Bench-13K Test Data Loading and Formatting\n",
    "\n",
    "This script is responsible for loading multiple-choice questions (MCQs) from JSONL benchmark files and formatting them for use in LLM inference and evaluation.\n",
    "\n",
    "- The `load_jsonl_mcq(path, sample_size, difficulty)` function:\n",
    "  - Loads the question, options, and correct answer index.\n",
    "  - Randomly samples a specified number of questions.\n",
    "  - Converts each question into a dictionary containing `query`, `answer`, and `difficulty`.\n",
    "\n",
    "- The `extract_predicted_option(text)` function is designed to extract answer choices (e.g., A/B/C/D or 1/2/3/4) from the model-generated output.\n",
    "  - Example: `\"The answer is C.\"` â†’ `\"C\"`\n",
    "\n",
    "- The loaded test samples are stored in `test_cases_all`. In the current setup, only `fin_E.json` is used with 50 questions.\n",
    "\n",
    "âœ… Test case loading is complete and ready for integration with LLM inference and accuracy evaluation workflows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f639ef93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å»ºç«‹ test_cases_allï¼Œå…± 50 é¡Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl_mcq(path, sample_size=100, difficulty=None):\n",
    "    \"\"\"å¾ JSONL æª”æ¡ˆè¼‰å…¥é¸æ“‡é¡Œæ ¼å¼è³‡æ–™ï¼Œè½‰æˆ query-answer dict\"\"\"\n",
    "    raw = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            raw.append(json.loads(line.strip()))\n",
    "\n",
    "    sample_size = min(len(raw), sample_size)\n",
    "    sampled = random.sample(raw, sample_size)\n",
    "\n",
    "    formatted = []\n",
    "    for item in sampled:\n",
    "        question = item[0]\n",
    "        options = item[1]\n",
    "        answer_index = int(item[2]) - 1\n",
    "        answer_text = options[answer_index]\n",
    "        formatted.append({\n",
    "            \"query\": question,\n",
    "            \"answer\": answer_text,\n",
    "            \"difficulty\": difficulty \n",
    "        })\n",
    "\n",
    "    return formatted\n",
    "\n",
    "def extract_predicted_option(text):\n",
    "    # å˜—è©¦å¾ç”Ÿæˆæ–‡å­—ä¸­æŠ“å‡º A/B/C/D æˆ– 1/2/3/4\n",
    "    for opt in [\"A\", \"B\", \"C\", \"D\", \"E\", \"1\", \"2\", \"3\", \"4\"]:\n",
    "        if f\"{opt})\" in text or f\"{opt}.\" in text or f\"The answer is {opt}\" in text:\n",
    "            return opt\n",
    "    return \"?\"\n",
    "\n",
    "# è©•ä¼°å·¥å…·\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "meteor = evaluate.load(\"meteor\")\n",
    "\n",
    "BENCHMARK_E_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_E.json\"\n",
    "# BENCHMARK_M_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_M.json\"\n",
    "# BENCHMARK_H_PATH = \"branchmark/ORAN-Bench-13K/Benchmark/fin_H.json\"\n",
    "\n",
    "# å„å– 100 é¡Œï¼Œä¸¦åŠ ä¸Šé›£åº¦æ¨™è¨˜\n",
    "test_cases_E = load_jsonl_mcq(BENCHMARK_E_PATH, sample_size=50, difficulty=\"E\")\n",
    "# test_cases_M = load_jsonl_mcq(BENCHMARK_M_PATH, sample_size=100, difficulty=\"M\")\n",
    "# test_cases_H = load_jsonl_mcq(BENCHMARK_H_PATH, sample_size=100, difficulty=\"H\")\n",
    "\n",
    "# åˆä½µç¸½æ¸¬è³‡ï¼ˆå…± 300 é¡Œï¼‰\n",
    "# test_cases_all = test_cases_E + test_cases_M + test_cases_H\n",
    "test_cases_all = test_cases_E\n",
    "print(f\"âœ… å·²å»ºç«‹ test_cases_allï¼Œå…± {len(test_cases_all)} é¡Œ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d825059",
   "metadata": {},
   "source": [
    "## TeleQnA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b3c441a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Total questions loaded: 10000\n",
      "\n",
      "ğŸ§ª Sample 1\n",
      "Q: What is the purpose of the fronthaul links in a CF (Cell-free) mMIMO (massive multiple-input-multiple-output) network?\n",
      "A: option 1: To connect the APs to the CPUs\n",
      "\n",
      "ğŸ§ª Sample 2\n",
      "Q: What modulation format is also referred to as on-off keying (OOK)?\n",
      "A: option 4: Amplitude shift keying (ASK)\n",
      "\n",
      "ğŸ§ª Sample 3\n",
      "Q: What are the two categories of channel estimation methods?\n",
      "A: option 1: Training-based and non-training-based\n",
      "âœ… å·²éš¨æ©ŸæŒ‘é¸ 100 é¡Œæ¸¬è©¦é¡Œç›®\n"
     ]
    }
   ],
   "source": [
    "# Load question-answer pairs\n",
    "with open(\"/home/aiml/johnson/thesis_rag/branchmark/TeleQnA/TeleQnA.txt\", encoding=\"utf-8\") as f:\n",
    "    all_qa = json.load(f)\n",
    "\n",
    "# æ ¼å¼æ•´ç†\n",
    "qa_list = []\n",
    "for item in all_qa.values():\n",
    "    qa_list.append({\n",
    "        \"query\": item[\"question\"],\n",
    "        \"option 1\": item.get(\"option 1\", \"\"),\n",
    "        \"option 2\": item.get(\"option 2\", \"\"),\n",
    "        \"option 3\": item.get(\"option 3\", \"\"),\n",
    "        \"option 4\": item.get(\"option 4\", \"\"),\n",
    "        \"option 5\": item.get(\"option 5\", \"\"),\n",
    "        \"answer\": item[\"answer\"]\n",
    "    })\n",
    "\n",
    "\n",
    "# é¡¯ç¤ºç¸½é¡Œæ•¸\n",
    "print(f\"ğŸ“Š Total questions loaded: {len(qa_list)}\")\n",
    "\n",
    "# éš¨æ©ŸæŒ‘é¸ 100 é¡Œï¼ˆä¸é‡è¤‡ï¼‰\n",
    "sample_size = 100\n",
    "sampled_qa = random.sample(qa_list, min(sample_size, len(qa_list)))\n",
    "# å¯é¸ï¼šç¢ºèªå‰ 3 é¡Œå…§å®¹\n",
    "for i, qa in enumerate(sampled_qa[:3]):\n",
    "    print(f\"\\nğŸ§ª Sample {i+1}\")\n",
    "    print(\"Q:\", qa[\"query\"])\n",
    "    print(\"A:\", qa[\"answer\"])\n",
    "    \n",
    "test_cases_all = sampled_qa\n",
    "print(f\"âœ… å·²éš¨æ©ŸæŒ‘é¸ {len(sampled_qa)} é¡Œæ¸¬è©¦é¡Œç›®\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a119b893",
   "metadata": {},
   "source": [
    "## no_RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da24921b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é–‹å§‹åŸ·è¡Œ RAG æ¨è«–ï¼Œå…± 1 é¡Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Non-RAG outputs saved to /home/aiml/johnson/thesis_rag/output/no_rag_outputs.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"âœ… é–‹å§‹åŸ·è¡Œ RAG æ¨è«–ï¼Œå…± {len(sampled_qa)} é¡Œ\")\n",
    "output_path = \"/home/aiml/johnson/thesis_rag/output/no_rag_outputs.txt\"\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, device=0)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "hf_pipeline = llm.pipeline \n",
    "\n",
    "# æ‰¹æ¬¡è·‘ non-RAG\n",
    "batch_size = 10\n",
    "no_rag_outputs = []\n",
    "for i in tqdm(range(0, len(test_cases_all), batch_size)):\n",
    "    batch_queries = [build_prompt_with_options(case) for case in sampled_qa]\n",
    "    batch_outputs = llm.pipeline(batch_queries)\n",
    "\n",
    "    for r in batch_outputs:\n",
    "        if isinstance(r, list) and isinstance(r[0], dict) and \"generated_text\" in r[0]:\n",
    "            no_rag_outputs.append(r[0][\"generated_text\"])\n",
    "        elif isinstance(r, dict) and \"generated_text\" in r:\n",
    "            no_rag_outputs.append(r[\"generated_text\"])\n",
    "        elif isinstance(r, str):\n",
    "            no_rag_outputs.append(r)\n",
    "        else:\n",
    "            print(\"âš ï¸ Unexpected format:\", r)\n",
    "            no_rag_outputs.append(\"<<PARSE ERROR>>\")\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in no_rag_outputs:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Non-RAG outputs saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c521c7e",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1226fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é–‹å§‹åŸ·è¡Œ RAG æ¨è«–ï¼Œå…± 1 é¡Œ\n",
      "{'query': 'What is the responsibility of the network slice broker (NSB)?', 'option 1': 'To orchestrate and manage the US-NSIs', 'option 2': 'To securely expose required services and capabilities to every US-NSI', 'option 3': 'To dynamically allocate network resources to every US-NSI', 'option 4': \"To translate tenant requirements to US-NSIs' requirements\", 'option 5': 'To enable the tenant to dynamically request and lease resources from the operator', 'answer': 'option 5: To enable the tenant to dynamically request and lease resources from the operator'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 26.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Error on case 0: Missing some input keys: {'option_4', 'option_2', 'option_3', 'option_5', 'option_1'}\n",
      "âœ… RAG outputs saved to /home/aiml/johnson/thesis_rag/output/rag_outputs.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "rag_outputs = []\n",
    "output_path_rag = \"/home/aiml/johnson/thesis_rag/output/rag_outputs.txt\"\n",
    "\n",
    "print(f\"âœ… é–‹å§‹åŸ·è¡Œ RAG æ¨è«–ï¼Œå…± {len(sampled_qa)} é¡Œ\")\n",
    "print(sampled_qa[0])\n",
    "\n",
    "for i in tqdm(range(len(sampled_qa))):\n",
    "    case = sampled_qa[i]\n",
    "    input_dict = {\n",
    "    \"question\": case[\"query\"],\n",
    "    **{f\"option_{j}\": case.get(f\"option {j}\", \"\") for j in range(1, 6)}\n",
    "}\n",
    "    \n",
    "\n",
    "    try:\n",
    "        result = rag_chain.run(input_dict)\n",
    "        rag_outputs.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error on case {i}: {e}\")\n",
    "        rag_outputs.append(\"<<ERROR>>\")\n",
    "\n",
    "# å„²å­˜åˆ°æª”æ¡ˆ\n",
    "with open(output_path_rag, \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in rag_outputs:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "print(f\"âœ… RAG outputs saved to {output_path_rag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a1cb170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/aiml/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "bleu = load(\"bleu\")\n",
    "rouge = load(\"rouge\")\n",
    "meteor = load(\"meteor\")\n",
    "\n",
    "# é å‚™è³‡æ–™\n",
    "references = [case[\"answer\"] for case in test_cases_all]\n",
    "\n",
    "# æ‰¹æ¬¡è©•åˆ†\n",
    "bleu_norag = bleu.compute(predictions=no_rag_outputs, references=[[ref] for ref in references])[\"bleu\"]\n",
    "bleu_rag = bleu.compute(predictions=rag_outputs, references=[[ref] for ref in references])[\"bleu\"]\n",
    "\n",
    "rouge_norag = rouge.compute(predictions=no_rag_outputs, references=references)[\"rougeL\"]\n",
    "rouge_rag = rouge.compute(predictions=rag_outputs, references=references)[\"rougeL\"]\n",
    "\n",
    "meteor_norag = meteor.compute(predictions=no_rag_outputs, references=references)[\"meteor\"]\n",
    "meteor_rag = meteor.compute(predictions=rag_outputs, references=references)[\"meteor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d153024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š BLEU\n",
      "  No RAG: 0.0010\n",
      "  RAG   : 0.0011\n",
      "ğŸ“Š ROUGE-L\n",
      "  No RAG: 0.0182\n",
      "  RAG   : 0.0255\n",
      "ğŸ“Š METEOR\n",
      "  No RAG: 0.0460\n",
      "  RAG   : 0.0620\n"
     ]
    }
   ],
   "source": [
    "print(\"ğŸ“Š BLEU\")\n",
    "print(f\"  No RAG: {bleu_norag:.4f}\")\n",
    "print(f\"  RAG   : {bleu_rag:.4f}\")\n",
    "print(\"ğŸ“Š ROUGE-L\")\n",
    "print(f\"  No RAG: {rouge_norag:.4f}\")\n",
    "print(f\"  RAG   : {rouge_rag:.4f}\")\n",
    "print(\"ğŸ“Š METEOR\")\n",
    "print(f\"  No RAG: {meteor_norag:.4f}\")\n",
    "print(f\"  RAG   : {meteor_rag:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cae9e916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Accuracy (No RAG): 10.00%\n",
      "âœ… Accuracy (RAG): 3.00%\n"
     ]
    }
   ],
   "source": [
    "acc_norag = 0\n",
    "acc_rag = 0\n",
    "\n",
    "for i, case in enumerate(test_cases_all):\n",
    "    gold = case[\"answer\"]\n",
    "    pred_no_rag = extract_predicted_option(no_rag_outputs[i])\n",
    "    pred_rag = extract_predicted_option(rag_outputs[i])\n",
    "\n",
    "    if pred_no_rag in gold:\n",
    "        acc_norag += 1\n",
    "    if pred_rag in gold:\n",
    "        acc_rag += 1\n",
    "\n",
    "acc_norag /= len(test_cases_all)\n",
    "acc_rag /= len(test_cases_all)\n",
    "\n",
    "print(f\"âœ… Accuracy (No RAG): {acc_norag:.2%}\")\n",
    "print(f\"âœ… Accuracy (RAG): {acc_rag:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
