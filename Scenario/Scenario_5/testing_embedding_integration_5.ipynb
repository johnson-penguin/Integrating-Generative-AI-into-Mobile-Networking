{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 指定 log 檔案\n",
    "du_log_file = \"/home/aiml/johnson/Scenario/Scenario_5/DU/log/Scenario_5.log\"\n",
    "ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_5/RU/log/RU.log\"\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/Scenario/Scenario_5/FH/fh.pcap\"\n",
    "debug_yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "reference_context_path = \"/home/aiml/johnson/Scenario/Scenario_5/reference_config.txt\"\n",
    "\n",
    "current_config_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5.conf\"\n",
    "current_config_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5.conf.segments.json\"\n",
    "\n",
    "rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\"\n",
    "rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\"\n",
    "\n",
    "none_rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\"\n",
    "none_rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( debug_yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 將每一筆資料嵌入的格式（以 symptom + log 為主）\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\\n\"\n",
    "\n",
    "    if \"notes\" in item and item[\"notes\"]:\n",
    "        content += f\"Notes: {item['notes']}\\n\"\n",
    "\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # ✅ Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4005900/3660683362.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "2025-04-30 02:50:55.197207: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 02:50:55.216676: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-30 02:50:55.216698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-30 02:50:55.217250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-30 02:50:55.220552: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 02:50:55.599149: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Debug embedding 建立完成並已儲存\n",
      "📦 總筆數： 20\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 246ecc3e-c1b9-42d0-ae25-bb6dc1facc85\n",
      "Document Text: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea4'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown ciphering algorithm \"nia4\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred ciphering algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "\n",
      "\n",
      "Metadata: {'related_config': 'ciphering_algorithm', 'stage': 'gnb_initialization'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: a511669a-f851-409b-9db2-14bf4be21d05\n",
      "Document Text: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "Metadata: {'related_config': 'integrity_algorithms', 'stage': 'gnb_initialization'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4005900/3660683362.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 你也可以改用 Gemini 或 OpenAI embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 將文本嵌入向量並存入 Chroma 資料庫\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"✅ Debug embedding 建立完成並已儲存\")\n",
    "\n",
    "\n",
    "\n",
    "# 檢查嵌入總筆數\n",
    "print(\"📦 總筆數：\", vectordb._collection.count())\n",
    "# 顯示前幾筆嵌入資料內容（包括原始文本與 metadata）\n",
    "peek_data = vectordb._collection.get(limit=2)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RU LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO RU log (file not found)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_ru_log_info(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"NO RU log (file not found)\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"NO RU log (error reading file: {e})\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    # Step 1: 優先找 ERNO 與其後續錯誤描述\n",
    "    erno_pattern = re.compile(r\"ERNO 0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\")\n",
    "    error_blocks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if erno_pattern.search(lines[i]):\n",
    "            block = [lines[i]]\n",
    "            ts_prefix = lines[i][:17]  # timestamp 開頭\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                if lines[i].startswith(ts_prefix) or \">>>Oran\" in lines[i] or \" at line \" in lines[i]:\n",
    "                    block.append(lines[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            error_blocks.append(\"\".join(block))\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if error_blocks:\n",
    "        print(\"=== ERNO Errors with Traceback ===\")\n",
    "        return \"\\n\".join(error_blocks)\n",
    "\n",
    "    # Step 2: 擷取 RX-WINDOW-STATS + RX-WINDOW-TIMING 整段\n",
    "    stats_started = False\n",
    "    timing_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"RX-WINDOW-STATS\" in line:\n",
    "            stats_started = True\n",
    "        if stats_started:\n",
    "            timing_block.append(line)\n",
    "            if \"RX_LATEST_C_DL\" in line:\n",
    "                break  # 到 timing 最後一項為止\n",
    "\n",
    "    print(\"=== Timing Window Extracted ===\")\n",
    "    return \"\".join(timing_block)\n",
    "\n",
    "log_result = extract_ru_log_info(ru_log_file)\n",
    "\n",
    "query = log_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_5/FH/fh.pcap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (pkt_data, pkt_metadata) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mRawPcapReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcap_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m     packet_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m c_plane_count \u001b[38;5;241m=\u001b[39m count_plane_packets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-Plane\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1385\u001b[0m, in \u001b[0;36mPcapReader_metaclass.__call__\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a cls instance, use the `alternative` if that\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m )\n\u001b[0;32m-> 1385\u001b[0m filename, fdesc, magic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Scapy_Exception(\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data could be read!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1419\u001b[0m, in \u001b[0;36mPcapReader_metaclass.open\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1418\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fname\n\u001b[0;32m-> 1419\u001b[0m     fdesc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: _ByteStream\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m     magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1f\u001b[39;00m\u001b[38;5;130;01m\\x8b\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;66;03m# GZIP header detected.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_5/FH/fh.pcap'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "\n",
    "# pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/normal.pcap\"\n",
    "\n",
    "packet_count = 0\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "def get_packet_count(pcap_file):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_file, \"-q\", \"-z\", \"io,stat,0\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    for line in result.stdout.splitlines():\n",
    "        if \"|   \" in line and \"Frames\" in line:\n",
    "            try:\n",
    "                fields = line.split(\"|\")\n",
    "                frame_info = fields[2].strip()  # Ex: \"X frames\"\n",
    "                frame_count = int(frame_info.split()[0])\n",
    "                return frame_count\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "    packet_count += 1\n",
    "\n",
    "c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "print(\"Control_Plane_Packets: \", c_plane_count)\n",
    "print(\"User_Plane_Packets: \", u_plane_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DU log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found matching log for stage [gnb_initialization]: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"去除ANSI控制字元 + 移除引號 + 去除多餘空格\"\"\"\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    s = ansi_escape.sub('', s)\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "if not Path(du_log_file).exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {du_log_file}\")\n",
    "if not Path(debug_yaml_path).exists():\n",
    "    raise FileNotFoundError(f\"Debug YAML not found: {debug_yaml_path}\")\n",
    "\n",
    "# 讀取 debug.yaml\n",
    "with open(debug_yaml_path, 'r', encoding='utf-8') as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 整理出 (log_snippet, stage) 對應\n",
    "target_entries = []\n",
    "for item in debug_data:\n",
    "    if 'log_snippet' in item:\n",
    "        snippet = item['log_snippet']\n",
    "        stage = item.get('stage', 'unknown')\n",
    "        if \";\" in snippet:\n",
    "            parts = [s.strip() for s in snippet.split(\";\")]\n",
    "            for p in parts:\n",
    "                target_entries.append((p, stage))\n",
    "        else:\n",
    "            target_entries.append((snippet.strip(), stage))\n",
    "\n",
    "# 搜索 log\n",
    "found_results = []\n",
    "with open(du_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for raw_line in f:\n",
    "        line = clean_text(raw_line)\n",
    "        for snippet, stage in target_entries:\n",
    "            snippet_cleaned = clean_text(snippet)\n",
    "            # 🔥 只要關鍵字部分包含就算符合\n",
    "            if snippet_cleaned in line:\n",
    "                found_results.append((stage, snippet))\n",
    "                \n",
    "# 輸出結果\n",
    "if found_results:\n",
    "    for stage, snippet in found_results:\n",
    "        print(f\"✅ Found matching log for stage [{stage}]: {snippet}\")\n",
    "        query = snippet\n",
    "else:\n",
    "    print(\"❌ No matching logs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "- Related config: integrity_algorithms\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "- Related config: integrity_algorithms\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_config_json_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "# RAG prompt_template\n",
    "# prompt_template = f\"\"\"\n",
    "# You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "# Issue Description:\n",
    "# \"{query}\"\n",
    "\n",
    "# Matching debug knowledge:\n",
    "# {matched_case.page_content}\n",
    "# Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "# Reference Device Address Table (external reference file):\n",
    "# {reference_context}\n",
    "\n",
    "# Current configuration block:\n",
    "# {config_segments_context}\n",
    "\n",
    "# Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "# Return a list of JSON objects with the following structure:\n",
    "# [\n",
    "#   {{\n",
    "#     \"label\": \"parameter_name\",\n",
    "#     \"content\": \"parameter_name = (...);\",\n",
    "#     \"reference_reason\": \"Short explanation matching the value to the reference device table (e.g., correct MAC, matches expected setting).\",\n",
    "#     \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
    "#   }},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# - Only include parameters listed in 'Relevant parameters'.\n",
    "# - Do not include any explanation outside of the JSON structure.\n",
    "# - Keep \"reference_reason\" based on the reference table.\n",
    "# - Derive \"model_reason\" from your own technical reasoning.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration to resolve the described issue based on your technical expertise.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"model_reason\": \"Technical explanation in 1-2 sentences explaining why this change is necessary, beneficial, or resolves the network issue.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "- Only revise parameters that are necessary to resolve the issue.\n",
    "- If no changes are needed, return an empty list: []\n",
    "- Strictly output only valid JSON without any additional text or explanation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.1-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0,\n",
    "#   top_p=0.7,\n",
    "#   max_tokens=1024,\n",
    "# )\n",
    "\n",
    "# for chunk in client.stream([{\"role\":\"user\",\"content\":\"\"}]): \n",
    "#   print(chunk.content, end=\"\")\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])\n",
    "# print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisions：\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"security\",\n",
      "    \"content\": \"security = {\\n  # preferred ciphering algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nea0, nea1, nea2, nea3\\n  ciphering_algorithms = ( \\\"nea4\\\" );\\n\\n  # preferred integrity algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nia0, nia1, nia2, nia3\\n  integrity_algorithms = ( \\\"nia2\\\");\\n\\n  # setting \\'drb_ciphering\\' to \\\"no\\\" disables ciphering for DRBs, no matter\\n  # what \\'ciphering_algorithms\\' configures; same thing for \\'drb_integrity\\'\\n  drb_ciphering = \\\"yes\\\";\\n  drb_integrity = \\\"no\\\";\\n};\",\n",
      "    \"model_reason\": \"The error message indicates that \\\"nia8\\\" is an unknown integrity algorithm. Removing \\\"nia9\\\" resolves the issue by ensuring that only valid integrity algorithms are configured. It's more prudent to remove nia9 and keep nia2, as it is more commonly supported.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt_template)                                # Gemini API\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisions：\\n\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])            # NIV\n",
    "# # LLM Suggested Revisions\n",
    "# print(\"LLM Suggested Revisions：\\n\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ JSON 解析失敗: Invalid \\escape: line 4 column 404 (char 434)\n",
      "🔍 原始內容:\n",
      " [\n",
      "  {\n",
      "    \"label\": \"security\",\n",
      "    \"content\": \"security = {\\n  # preferred ciphering algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nea0, nea1, nea2, nea3\\n  ciphering_algorithms = ( \\\"nea4\\\" );\\n\\n  # preferred integrity algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nia0, nia1, nia2, nia3\\n  integrity_algorithms = ( \\\"nia2\\\");\\n\\n  # setting \\'drb_ciphering\\' to \\\"no\\\" disables ciphering for DRBs, no matter\\n  # what \\'ciphering_algorithms\\' configures; same thing for \\'drb_integrity\\'\\n  drb_ciphering = \\\"yes\\\";\\n  drb_integrity = \\\"no\\\";\\n};\",\n",
      "    \"model_reason\": \"\\\"nia8\\\" and \\\"nia9\\\" are not valid integrity algorithms. Only \\\"nia0\\\", \\\"nia1\\\", \\\"nia2\\\", and \\\"nia3\\\" are valid. Removing nia9 since it is not valid will resolve the configuration error.\"\n",
      "  }\n",
      "]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"將 LLM 回傳的 markdown JSON 文字轉成 Python dict\"\"\"\n",
    "    # 移除 markdown 格式包裝\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # 開頭的 ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # 結尾的 ```\n",
    "\n",
    "    # 嘗試解析 JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON 解析失敗:\", e)\n",
    "        print(\"🔍 原始內容:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# ✅ 用法\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\n",
      "🛠️ Modified parameters：\n",
      " - integrity_algorithms\n",
      " - integrity_algorithms\n",
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\n",
      "🛠️ Modified parameters：\n",
      " - integrity_algorithms\n",
      " - integrity_algorithms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # 讀入原始 conf 檔案\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    modified_labels = []\n",
    "\n",
    "    # 依據每個 label 進行替換\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "\n",
    "        # 用正則表達式找原始設定行\n",
    "        pattern = rf\"{label}\\s*=\\s*[^;]+;\"\n",
    "        new_content, count = re.subn(pattern, replacement, content)\n",
    "\n",
    "\n",
    "        if count > 0:\n",
    "            modified_labels.append(label)\n",
    "            content = new_content  # 更新 content 為替換後版本\n",
    "        else:\n",
    "            print(f\"⚠️ No matching setting found ：{label}\")\n",
    "\n",
    "    # 寫入新的 conf 檔案\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"✅ Update file ：{output_path}\")\n",
    "    \n",
    "    # 顯示修改報告\n",
    "    if modified_labels:\n",
    "        print(\"🛠️ Modified parameters：\")\n",
    "        for label in modified_labels:\n",
    "            print(f\" - {label}\")\n",
    "    else:\n",
    "        print(\"📭 No parameters were modified\")\n",
    "\n",
    "# ✅ 執行範例\n",
    "apply_llm_suggestions(\n",
    "    conf_path  =current_config_path,\n",
    "    output_path=none_rag_after_conf_path, \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=current_config_json_path,\n",
    "    output_path=none_rag_after_json_path,\n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
