{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 指定 log 檔案\n",
    "du_log_file = \"/home/aiml/johnson/Scenario/Scenario_5/DU/log/Scenario_5.log\"\n",
    "ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_5/RU/log/RU.log\"\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/Scenario/Scenario_5/FH/fh.pcap\"\n",
    "debug_yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "reference_context_path = \"/home/aiml/johnson/Scenario/Scenario_5/reference_config.txt\"\n",
    "\n",
    "current_config_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5.conf\"\n",
    "current_config_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5.conf.segments.json\"\n",
    "\n",
    "rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\"\n",
    "rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\"\n",
    "\n",
    "none_rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\"\n",
    "none_rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( debug_yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 將每一筆資料嵌入的格式（以 symptom + log 為主）\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\\n\"\n",
    "\n",
    "    if \"notes\" in item and item[\"notes\"]:\n",
    "        content += f\"Notes: {item['notes']}\\n\"\n",
    "\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # ✅ Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4186352/3660683362.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "2025-05-01 13:16:04.091239: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-01 13:16:04.109866: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-05-01 13:16:04.109887: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-05-01 13:16:04.110413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-01 13:16:04.113917: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-01 13:16:04.491677: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Debug embedding 建立完成並已儲存\n",
      "📦 總筆數： 25\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 246ecc3e-c1b9-42d0-ae25-bb6dc1facc85\n",
      "Document Text: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea4'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown ciphering algorithm \"nia4\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred ciphering algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "\n",
      "\n",
      "Metadata: {'related_config': 'ciphering_algorithm', 'stage': 'gnb_initialization'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: a511669a-f851-409b-9db2-14bf4be21d05\n",
      "Document Text: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "Metadata: {'related_config': 'integrity_algorithms', 'stage': 'gnb_initialization'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4186352/3660683362.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 你也可以改用 Gemini 或 OpenAI embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 將文本嵌入向量並存入 Chroma 資料庫\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"✅ Debug embedding 建立完成並已儲存\")\n",
    "\n",
    "\n",
    "\n",
    "# 檢查嵌入總筆數\n",
    "print(\"📦 總筆數：\", vectordb._collection.count())\n",
    "# 顯示前幾筆嵌入資料內容（包括原始文本與 metadata）\n",
    "peek_data = vectordb._collection.get(limit=2)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RU LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO RU log (file not found)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_ru_log_info(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"NO RU log (file not found)\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"NO RU log (error reading file: {e})\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    # Step 1: 優先找 ERNO 與其後續錯誤描述\n",
    "    erno_pattern = re.compile(r\"ERNO 0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\")\n",
    "    error_blocks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if erno_pattern.search(lines[i]):\n",
    "            block = [lines[i]]\n",
    "            ts_prefix = lines[i][:17]  # timestamp 開頭\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                if lines[i].startswith(ts_prefix) or \">>>Oran\" in lines[i] or \" at line \" in lines[i]:\n",
    "                    block.append(lines[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            error_blocks.append(\"\".join(block))\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if error_blocks:\n",
    "        print(\"=== ERNO Errors with Traceback ===\")\n",
    "        return \"\\n\".join(error_blocks)\n",
    "\n",
    "    # Step 2: 擷取 RX-WINDOW-STATS + RX-WINDOW-TIMING 整段\n",
    "    stats_started = False\n",
    "    timing_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"RX-WINDOW-STATS\" in line:\n",
    "            stats_started = True\n",
    "        if stats_started:\n",
    "            timing_block.append(line)\n",
    "            if \"RX_LATEST_C_DL\" in line:\n",
    "                break  # 到 timing 最後一項為止\n",
    "\n",
    "    print(\"=== Timing Window Extracted ===\")\n",
    "    return \"\".join(timing_block)\n",
    "\n",
    "log_result = extract_ru_log_info(ru_log_file)\n",
    "\n",
    "query = log_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no file\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "def get_packet_count(pcap_file):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_file, \"-q\", \"-z\", \"io,stat,0\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    for line in result.stdout.splitlines():\n",
    "        if \"|   \" in line and \"Frames\" in line:\n",
    "            try:\n",
    "                fields = line.split(\"|\")\n",
    "                frame_info = fields[2].strip()  # Ex: \"X frames\"\n",
    "                frame_count = int(frame_info.split()[0])\n",
    "                return frame_count\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "# === 檢查檔案是否存在 ===\n",
    "if not os.path.exists(pcap_path):\n",
    "    print(\"no file\")\n",
    "    exit(1)\n",
    "else:\n",
    "    packet_count = 0\n",
    "    # Count total packets\n",
    "    for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "        packet_count += 1\n",
    "\n",
    "    # Count C-Plane and U-Plane packets using tshark\n",
    "    c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "    u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "\n",
    "    print(\"Control_Plane_Packets:\", c_plane_count)\n",
    "    print(\"User_Plane_Packets:\", u_plane_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DU log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found matching log for stage [gnb_initialization]: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"去除ANSI控制字元 + 移除引號 + 去除多餘空格\"\"\"\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    s = ansi_escape.sub('', s)\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "if not Path(du_log_file).exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {du_log_file}\")\n",
    "if not Path(debug_yaml_path).exists():\n",
    "    raise FileNotFoundError(f\"Debug YAML not found: {debug_yaml_path}\")\n",
    "\n",
    "# 讀取 debug.yaml\n",
    "with open(debug_yaml_path, 'r', encoding='utf-8') as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 整理出 (log_snippet, stage) 對應\n",
    "target_entries = []\n",
    "for item in debug_data:\n",
    "    if 'log_snippet' in item:\n",
    "        snippet = item['log_snippet']\n",
    "        stage = item.get('stage', 'unknown')\n",
    "        if \";\" in snippet:\n",
    "            parts = [s.strip() for s in snippet.split(\";\")]\n",
    "            for p in parts:\n",
    "                target_entries.append((p, stage))\n",
    "        else:\n",
    "            target_entries.append((snippet.strip(), stage))\n",
    "\n",
    "# 搜索 log\n",
    "found_results = []\n",
    "with open(du_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for raw_line in f:\n",
    "        line = clean_text(raw_line)\n",
    "        for snippet, stage in target_entries:\n",
    "            snippet_cleaned = clean_text(snippet)\n",
    "            # 🔥 只要關鍵字部分包含就算符合\n",
    "            if snippet_cleaned in line:\n",
    "                found_results.append((stage, snippet))\n",
    "                \n",
    "# 輸出結果\n",
    "if found_results:\n",
    "    for stage, snippet in found_results:\n",
    "        print(f\"✅ Found matching log for stage [{stage}]: {snippet}\")\n",
    "        query = snippet\n",
    "else:\n",
    "    print(\"❌ No matching logs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured integrity algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "- Related config: integrity_algorithms\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea8'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown integrity algorithm \"nia8\" in section \"security\" of the configuration file\n",
      "Notes: 1. Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "2. It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "3. Preferred integrity algorithms the first one of the list that an UE supports in chosen : Valid values: nia0, nia1, nia2, nia3\n",
      "4. The integrity_algorithms parameter supports configuring multiple algorithms as a list, for example: (\"nia2\", \"nia1\", \"nia3\", \"nia0\"). During security negotiation, the gNB will select the first algorithm from the list that is also supported by the UE.\n",
      "\n",
      "\n",
      "- Related config: integrity_algorithms\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_config_json_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "# RAG prompt_template\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Matching debug knowledge:\n",
    "{matched_case.page_content}\n",
    "Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "Reference Device Address Table (external reference file):\n",
    "{reference_context}\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"reference_reason\": \"Short explanation matching the value to the reference device table (e.g., correct MAC, matches expected setting).\",\n",
    "    \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "- Only include parameters listed in 'Relevant parameters'.\n",
    "- Do not include any explanation outside of the JSON structure.\n",
    "- Keep \"reference_reason\" based on the reference table.\n",
    "- Derive \"model_reason\" from your own technical reasoning.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# prompt_template = f\"\"\"\n",
    "# You are a 5G network expert. Your job is to revise configuration files based on observed network issues.\n",
    "\n",
    "# Issue Description:\n",
    "# \"{query}\"\n",
    "\n",
    "# Current configuration block:\n",
    "# {config_segments_context}\n",
    "\n",
    "# Please revise the configuration to resolve the described issue based on your technical expertise.\n",
    "\n",
    "# Return a list of JSON objects with the following structure:\n",
    "# [\n",
    "#   {{\n",
    "#     \"label\": \"parameter_name\",\n",
    "#     \"content\": \"parameter_name = (...);\",\n",
    "#     \"model_reason\": \"Technical explanation in 1-2 sentences explaining why this change is necessary, beneficial, or resolves the network issue.\"\n",
    "#   }},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# - Only revise parameters that are necessary to resolve the issue.\n",
    "# - If no changes are needed, return an empty list: []\n",
    "# - Strictly output only valid JSON without any additional text or explanation.\n",
    "# \"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.1-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0,\n",
    "#   top_p=0.7,\n",
    "#   max_tokens=1024,\n",
    "# )\n",
    "\n",
    "# for chunk in client.stream([{\"role\":\"user\",\"content\":\"\"}]): \n",
    "#   print(chunk.content, end=\"\")\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])\n",
    "# print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisions：\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"security\",\n",
      "    \"content\": \"security = {\\n  # preferred ciphering algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nea0, nea1, nea2, nea3\\n  ciphering_algorithms = ( \\\"nea4\\\" );\\n\\n  # preferred integrity algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nia0, nia1, nia2, nia3\\n  integrity_algorithms = ( \\\"nia2\\\" );\\n\\n  # setting \\'drb_ciphering\\' to \\\"no\\\" disables ciphering for DRBs, no matter\\n  # what \\'ciphering_algorithms\\' configures; same thing for \\'drb_integrity\\'\\n  drb_ciphering = \\\"yes\\\";\\n  drb_integrity = \\\"no\\\";\\n};\",\n",
      "    \"model_reason\": \"The error message indicates that \\\"nia8\\\" is an unknown integrity algorithm. Removing \\\"nia9\\\" resolves the error and allows the gNB to start correctly if UE supports nia2\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt_template)                                # Gemini API\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisions：\\n\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])            # NIV\n",
    "# # LLM Suggested Revisions\n",
    "# print(\"LLM Suggested Revisions：\\n\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ JSON 解析失敗: Invalid \\escape: line 4 column 405 (char 435)\n",
      "🔍 原始內容:\n",
      " [\n",
      "  {\n",
      "    \"label\": \"security\",\n",
      "    \"content\": \"security = {\\n  # preferred ciphering algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nea0, nea1, nea2, nea3\\n  ciphering_algorithms = ( \\\"nea4\\\" );\\n\\n  # preferred integrity algorithms\\n  # the first one of the list that an UE supports in chosen\\n  # valid values: nia0, nia1, nia2, nia3\\n  integrity_algorithms = ( \\\"nia2\\\" );\\n\\n  # setting \\'drb_ciphering\\' to \\\"no\\\" disables ciphering for DRBs, no matter\\n  # what \\'ciphering_algorithms\\' configures; same thing for \\'drb_integrity\\'\\n  drb_ciphering = \\\"yes\\\";\\n  drb_integrity = \\\"no\\\";\\n};\",\n",
      "    \"model_reason\": \"The error message indicates that \\\"nia8\\\" is an unknown integrity algorithm. Removing \\\"nia9\\\" resolves the error and allows the gNB to start correctly if UE supports nia2\"\n",
      "  }\n",
      "]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"將 LLM 回傳的 markdown JSON 文字轉成 Python dict\"\"\"\n",
    "    # 移除 markdown 格式包裝\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # 開頭的 ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # 結尾的 ```\n",
    "\n",
    "    # 嘗試解析 JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON 解析失敗:\", e)\n",
    "        print(\"🔍 原始內容:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# ✅ 用法\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf\n",
      "🛠️ Modified parameters：\n",
      " - integrity_algorithms\n",
      " - integrity_algorithms\n",
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_5/DU/conf/Scenario_5_modification_1.conf.segments.json\n",
      "🛠️ Modified parameters：\n",
      " - integrity_algorithms\n",
      " - integrity_algorithms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # 讀入原始 conf 檔案\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    modified_labels = []\n",
    "\n",
    "    # 依據每個 label 進行替換\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "\n",
    "        # 用正則表達式找原始設定行\n",
    "        pattern = rf\"{label}\\s*=\\s*[^;]+;\"\n",
    "        new_content, count = re.subn(pattern, replacement, content)\n",
    "\n",
    "\n",
    "        if count > 0:\n",
    "            modified_labels.append(label)\n",
    "            content = new_content  # 更新 content 為替換後版本\n",
    "        else:\n",
    "            print(f\"⚠️ No matching setting found ：{label}\")\n",
    "\n",
    "    # 寫入新的 conf 檔案\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"✅ Update file ：{output_path}\")\n",
    "    \n",
    "    # 顯示修改報告\n",
    "    if modified_labels:\n",
    "        print(\"🛠️ Modified parameters：\")\n",
    "        for label in modified_labels:\n",
    "            print(f\" - {label}\")\n",
    "    else:\n",
    "        print(\"📭 No parameters were modified\")\n",
    "\n",
    "# ✅ 執行範例\n",
    "apply_llm_suggestions(\n",
    "    conf_path  =current_config_path,\n",
    "    output_path=none_rag_after_conf_path, \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=current_config_json_path,\n",
    "    output_path=none_rag_after_json_path,\n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
