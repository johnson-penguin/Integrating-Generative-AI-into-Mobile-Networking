{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "\n",
    "# æŒ‡å®š log æª”æ¡ˆ\n",
    "du_log_file = \"/home/aiml/johnson/Scenario/Scenario_3/DU/log/Scenario_3.log\"\n",
    "ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_3/RU/log/RU.log\"\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap\"\n",
    "\n",
    "reference_context_path = \"/home/aiml/johnson/Scenario/Scenario_3/reference_config.txt\"\n",
    "\n",
    "current_config_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3.conf\"\n",
    "current_config_json_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3.conf.segments.json\"\n",
    "\n",
    "after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf\"\n",
    "after_json_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf.segments.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "with open( yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# å°‡æ¯ä¸€ç­†è³‡æ–™åµŒå…¥çš„æ ¼å¼ï¼ˆä»¥ symptom + log ç‚ºä¸»ï¼‰\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\"\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # âœ… Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš ï¸ ç„¡æ–°å¢è³‡æ–™ï¼Œçš†ç‚ºé‡è¤‡é …ç›®\n",
      "ğŸ“¦ ç¸½ç­†æ•¸ï¼š 6\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 2490cd60-a3d3-4485-bbcc-dfb55ee1d0d9\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured â€“ possible mismatch in MAC address between DU and RU\n",
      "Metadata: {'related_config': 'ru_addr, du_addr', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: 41214951-d646-4f62-bfcd-23f34c016659\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "Metadata: {'related_config': 'iq_width_prach, iq_width', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 3 ---\n",
      "Document ID: b5d11373-c074-43a7-8291-24afb143921c\n",
      "Document Text: Stage: ng_setup\n",
      "Symptom: Incorrect AMF IP configuration, unable to establish NG connection\n",
      "Log: [SCTP] Connect failed: Connection refused\n",
      "Metadata: {'related_config': 'amf_ip_address', 'stage': 'ng_setup'}\n",
      "\n",
      "--- Entry 4 ---\n",
      "Document ID: ea71ada9-c35c-4369-a78b-5fc09bd473f5\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured â€“ possible mismatch in MAC address between DU and RU\n",
      "Metadata: {'related_config': 'ru_addr, du_addr', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 5 ---\n",
      "Document ID: 4f6b79f2-69a6-4866-ad42-48abbb2aa8b4\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "Metadata: {'related_config': 'iq_width_prach, iq_width', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 6 ---\n",
      "Document ID: 3df4bc2f-a293-434d-b298-40c5b9e1bbba\n",
      "Document Text: Stage: ng_setup\n",
      "Symptom: Incorrect AMF IP configuration, unable to establish NG connection\n",
      "Log: [SCTP] Connect failed: Connection refused\n",
      "Metadata: {'related_config': 'amf_ip_address', 'stage': 'ng_setup'}\n"
     ]
    }
   ],
   "source": [
    "# ä½ å¯ä»¥æ”¹æˆ Gemini æˆ– OpenAI Embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# è¼‰å…¥ embedding_docsï¼Œå–å‡º texts èˆ‡ metadata\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "# åˆå§‹åŒ– Chroma å‘é‡è³‡æ–™åº«ï¼ˆè‹¥å·²å­˜åœ¨æœƒè®€å–ï¼‰\n",
    "vectordb = Chroma(persist_directory=\"./error_db\", embedding_function=embedding)\n",
    "\n",
    "# Step 1ï¸âƒ£ å–å¾—å·²å­˜åœ¨çš„ documents åšæ¯”å°\n",
    "existing = vectordb._collection.get()\n",
    "existing_docs = set(existing[\"documents\"]) if \"documents\" in existing else set()\n",
    "\n",
    "# Step 2ï¸âƒ£ éæ¿¾æ‰é‡è¤‡çš„å…§å®¹\n",
    "filtered_texts = []\n",
    "filtered_metadatas = []\n",
    "\n",
    "for text, meta in zip(texts, metadatas):\n",
    "    if text not in existing_docs:\n",
    "        filtered_texts.append(text)\n",
    "        filtered_metadatas.append(meta)\n",
    "\n",
    "# Step 3ï¸âƒ£ åƒ…å„²å­˜éé‡è¤‡éƒ¨åˆ†\n",
    "if filtered_texts:\n",
    "    vectordb.add_texts(filtered_texts, metadatas=filtered_metadatas)\n",
    "    vectordb.persist()\n",
    "    print(f\"âœ… æ–°å¢ {len(filtered_texts)} ç­†éé‡è¤‡åµŒå…¥è³‡æ–™\")\n",
    "else:\n",
    "    print(\"âš ï¸ ç„¡æ–°å¢è³‡æ–™ï¼Œçš†ç‚ºé‡è¤‡é …ç›®\")\n",
    "\n",
    "# Optional: æª¢æŸ¥å„²å­˜ç‹€æ…‹\n",
    "print(\"ğŸ“¦ ç¸½ç­†æ•¸ï¼š\", vectordb._collection.count())\n",
    "\n",
    "\n",
    "# é¡¯ç¤ºå‰å¹¾ç­†åµŒå…¥è³‡æ–™å…§å®¹ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡æœ¬èˆ‡ metadataï¼‰\n",
    "peek_data = vectordb._collection.get(limit=6)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RU LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERNO Errors with Traceback ===\n",
      "29/04/25 13:51:48:073\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:48:073\t >>>FhsDl_0_NR_I0: Received Payloadsize 1337 is higher than Expected Payloadsize 1492 \" in ../tm_oran_app/oran_ru/oran_ru_dl_cuplane/code/oran_ru_dl_cuplane.cpp at line 1275\n",
      "\n",
      "29/04/25 13:51:53:162\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:162\t >>>OranRxEth_FE_0_I0: m_RngBuf full.\" in ../tm_oran_app/oran_ru/oran_ru_fifos/interface/oran_uplane_fifo.hpp at line 238\n",
      "29/04/25 13:51:53:218\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:218\t >>>OranRxEth_FE_0_I0: m_RngBuf full.\" in ../tm_oran_app/oran_ru/oran_ru_fifos/interface/oran_uplane_fifo.hpp at line 238\n",
      "29/04/25 13:51:53:258\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:258\t >>>OranRxEth_FE_1_I0: m_RngBuf full.\" in ../tm_oran_app/oran_ru/oran_ru_fifos/interface/oran_uplane_fifo.hpp at line 238\n",
      "29/04/25 13:51:53:292\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:292\t >>>OranRxEth_FE_1_I0: m_RngBuf full.\" in ../tm_oran_app/oran_ru/oran_ru_fifos/interface/oran_uplane_fifo.hpp at line 238\n",
      "29/04/25 13:51:53:330\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:330\t >>>FhsDl_1_NR_I0: Received Payloadsize 1337 is higher than Expected Payloadsize 1492 \" in ../tm_oran_app/oran_ru/oran_ru_dl_cuplane/code/oran_ru_dl_cuplane.cpp at line 1275\n",
      "29/04/25 13:51:53:362\tI: ERNO 0x00000002 0x00000000 0x00000000 Assert Fail: \"\n",
      "29/04/25 13:51:53:362\t >>>FhsTimer_Main_I0: CRuDlCuPlane::PushWindowStartTimerEvent: Failed to queue m_WindowValidTimerEventQueue\" in ../tm_oran_app/oran_ru/oran_ru_dl_cuplane/code/oran_ru_dl_cuplane.cpp at line 1293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_ru_log_info(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Step 1: å„ªå…ˆæ‰¾ ERNO èˆ‡å…¶å¾ŒçºŒéŒ¯èª¤æè¿°\n",
    "    erno_pattern = re.compile(r\"ERNO 0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\")\n",
    "    error_blocks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if erno_pattern.search(lines[i]):\n",
    "            block = [lines[i]]\n",
    "            ts_prefix = lines[i][:17]  # timestamp é–‹é ­\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                if lines[i].startswith(ts_prefix) or \">>>Oran\" in lines[i] or \" at line \" in lines[i]:\n",
    "                    block.append(lines[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            error_blocks.append(\"\".join(block))\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if error_blocks:\n",
    "        print(\"=== ERNO Errors with Traceback ===\")\n",
    "        return \"\\n\".join(error_blocks)\n",
    "\n",
    "    # Step 2: æ“·å– RX-WINDOW-STATS + RX-WINDOW-TIMING æ•´æ®µ\n",
    "    stats_started = False\n",
    "    timing_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"RX-WINDOW-STATS\" in line:\n",
    "            stats_started = True\n",
    "        if stats_started:\n",
    "            timing_block.append(line)\n",
    "            if \"RX_LATEST_C_DL\" in line:\n",
    "                break  # åˆ° timing æœ€å¾Œä¸€é …ç‚ºæ­¢\n",
    "\n",
    "    print(\"=== Timing Window Extracted ===\")\n",
    "    return \"\".join(timing_block)\n",
    "\n",
    "log_result = extract_ru_log_info(ru_log_file)\n",
    "print(log_result)\n",
    "\n",
    "query = log_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (pkt_data, pkt_metadata) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mRawPcapReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcap_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m     packet_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m c_plane_count \u001b[38;5;241m=\u001b[39m count_plane_packets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-Plane\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1385\u001b[0m, in \u001b[0;36mPcapReader_metaclass.__call__\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a cls instance, use the `alternative` if that\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m )\n\u001b[0;32m-> 1385\u001b[0m filename, fdesc, magic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Scapy_Exception(\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data could be read!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1419\u001b[0m, in \u001b[0;36mPcapReader_metaclass.open\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1418\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fname\n\u001b[0;32m-> 1419\u001b[0m     fdesc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: _ByteStream\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m     magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1f\u001b[39;00m\u001b[38;5;130;01m\\x8b\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;66;03m# GZIP header detected.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "\n",
    "# pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/normal.pcap\"\n",
    "\n",
    "packet_count = 0\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "def get_packet_count(pcap_file):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_file, \"-q\", \"-z\", \"io,stat,0\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    for line in result.stdout.splitlines():\n",
    "        if \"|   \" in line and \"Frames\" in line:\n",
    "            try:\n",
    "                fields = line.split(\"|\")\n",
    "                frame_info = fields[2].strip()  # Ex: \"X frames\"\n",
    "                frame_count = int(frame_info.split()[0])\n",
    "                return frame_count\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "    packet_count += 1\n",
    "\n",
    "c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "print(\"Control_Plane_Packets: \", c_plane_count)\n",
    "print(\"User_Plane_Packets: \", u_plane_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DU log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Not found: [SCTP]   Connect failed: Connection refused\n",
      "âŒ Not found: [NGAP]   Received unsuccessful result for SCTP association (3), instance 0, cnx_id 1\n",
      "âœ… Found: [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch\n",
      "âœ… Found: [PHY]   Received Time doesn't correspond to the time we think it is (frame mismatch\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# è¦æœå°‹çš„å…©å€‹ç›®æ¨™å­—ä¸²ï¼ˆæ³¨æ„é€™è£¡æ‹¿æ‰äº†æ§åˆ¶å­—å…ƒï¼Œåªæ¯”å°æœ‰æ„ç¾©çš„è¨Šæ¯ï¼‰\n",
    "target_messages = [\n",
    "    \"[SCTP]   Connect failed: Connection refused\",\n",
    "    \"[NGAP]   Received unsuccessful result for SCTP association (3), instance 0, cnx_id 1\",\n",
    "    \"[PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch\",\n",
    "    \"[PHY]   Received Time doesn't correspond to the time we think it is (frame mismatch\"\n",
    "]\n",
    "\n",
    "# ç¢ºèª log æª”æ¡ˆå­˜åœ¨\n",
    "if not Path(du_log_file).exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {du_log_file}\")\n",
    "\n",
    "# è®€å–æª”æ¡ˆä¸¦æª¢æŸ¥\n",
    "found_targets = {msg: False for msg in target_messages}\n",
    "\n",
    "with open(du_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        for target in target_messages:\n",
    "            if target in line:\n",
    "                found_targets[target] = True\n",
    "\n",
    "# é¡¯ç¤ºæª¢æŸ¥çµæœ\n",
    "for target, found in found_targets.items():\n",
    "    if found:\n",
    "        print(f\"âœ… Found: {target}\")\n",
    "        query = f\"Stage: {target}\"\n",
    "    else:\n",
    "        print(f\"âŒ Not found: {target}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "- Related config: iq_width_prach, iq_width\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "- Related config: iq_width_prach, iq_width\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_config_json_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Matching debug knowledge:\n",
    "{matched_case.page_content}\n",
    "Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "Reference Device Address Table (external reference file):\n",
    "{reference_context}\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"reference_reason\": \"Short explanation matching the value to the reference device table (e.g., correct MAC, matches expected setting).\",\n",
    "    \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "- Only include parameters listed in 'Relevant parameters'.\n",
    "- Do not include any explanation outside of the JSON structure.\n",
    "- Keep \"reference_reason\" based on the reference table.\n",
    "- Derive \"model_reason\" from your own technical reasoning.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API è¨­å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.1-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0,\n",
    "#   top_p=0.7,\n",
    "#   max_tokens=1024,\n",
    "# )\n",
    "\n",
    "# for chunk in client.stream([{\"role\":\"user\",\"content\":\"\"}]): \n",
    "#   print(chunk.content, end=\"\")\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])\n",
    "# print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisionsï¼š\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"iq_width_prach\",\n",
      "    \"content\": \"iq_width_prach = 9;\",\n",
      "    \"reference_reason\": \"Matches iq_width_prach value from reference_info.\",\n",
      "    \"model_reason\": \"The mismatch in frame timing suggests synchronization issues. Increasing the IQ width may improve timing resolution and reduce frame mismatches by providing more precise synchronization information.\"\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"iq_width\",\n",
      "    \"content\": \"iq_width = 9;\",\n",
      "    \"reference_reason\": \"Matches iq_width value from reference_info.\",\n",
      "    \"model_reason\": \"In addition to PRACH, adjusting the general iq_width may also improve the accuracy of signal processing and reduce synchronization errors, preventing frame counter jumps and double sync detections.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt_template)                                # Gemini API\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisionsï¼š\\n\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])            # NIV\n",
    "# # LLM Suggested Revisions\n",
    "# print(\"LLM Suggested Revisionsï¼š\\n\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'iq_width_prach', 'content': 'iq_width_prach = 9;'}, {'label': 'iq_width', 'content': 'iq_width = 9;'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"å°‡ LLM å›å‚³çš„ markdown JSON æ–‡å­—è½‰æˆ Python dict\"\"\"\n",
    "    # ç§»é™¤ markdown æ ¼å¼åŒ…è£\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # é–‹é ­çš„ ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # çµå°¾çš„ ```\n",
    "\n",
    "    # å˜—è©¦è§£æ JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ JSON è§£æå¤±æ•—:\", e)\n",
    "        print(\"ğŸ” åŸå§‹å…§å®¹:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# âœ… ç”¨æ³•\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Update file ï¼š/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf\n",
      "âœ… Update file ï¼š/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf.segments.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # è®€å…¥åŸå§‹ conf æª”æ¡ˆ\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # ä¾æ“šæ¯å€‹ label é€²è¡Œæ›¿æ›\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "        # ç”¨æ­£å‰‡è¡¨é”å¼æŠ“å‡ºå°æ‡‰çš„è¨­å®šè¡Œï¼Œä¸¦æ›¿æ›\n",
    "        pattern = rf\"{label}\\s*=\\s*\\(.*?\\);\"\n",
    "        content = re.sub(pattern, replacement, content)\n",
    "\n",
    "    # å¯«å…¥æ–°çš„ conf æª”æ¡ˆ\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"âœ… Update file ï¼š{output_path}\")\n",
    "\n",
    "# âœ… åŸ·è¡Œç¯„ä¾‹\n",
    "apply_llm_suggestions(\n",
    "    conf_path  =current_config_path,\n",
    "    output_path=after_conf_path, \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=current_config_json_path,\n",
    "    output_path=after_json_path,\n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
