{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 指定 log 檔案\n",
    "du_log_file = \"/home/aiml/johnson/Scenario/Scenario_3/DU/log/Scenario_3.log\"\n",
    "ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_3/RU/log/RU.log\"\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap\"\n",
    "debug_yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "reference_context_path = \"/home/aiml/johnson/Scenario/Scenario_3/reference_config.txt\"\n",
    "\n",
    "current_config_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3.conf\"\n",
    "current_config_json_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3.conf.segments.json\"\n",
    "\n",
    "rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf\"\n",
    "rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf.segments.json\"\n",
    "\n",
    "none_rag_after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf\"\n",
    "none_rag_after_json_path=\"/home/aiml/johnson/Scenario/Scenario_3/DU/conf/Scenario_3_modification_1.conf.segments.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( debug_yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 將每一筆資料嵌入的格式（以 symptom + log 為主）\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\\n\"\n",
    "\n",
    "    if \"notes\" in item and item[\"notes\"]:\n",
    "        content += f\"Notes: {item['notes']}\\n\"\n",
    "\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # ✅ Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4055832/3660683362.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "2025-04-30 03:03:07.887833: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 03:03:07.906439: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-30 03:03:07.906464: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-30 03:03:07.906972: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-30 03:03:07.910450: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 03:03:08.285577: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Debug embedding 建立完成並已儲存\n",
      "📦 總筆數： 12\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 2490cd60-a3d3-4485-bbcc-dfb55ee1d0d9\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured – possible mismatch in MAC address between DU and RU\n",
      "Metadata: {'related_config': 'ru_addr, du_addr', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: 41214951-d646-4f62-bfcd-23f34c016659\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "Metadata: {'related_config': 'iq_width_prach, iq_width', 'stage': 'fh_setup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4055832/3660683362.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 你也可以改用 Gemini 或 OpenAI embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 將文本嵌入向量並存入 Chroma 資料庫\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"✅ Debug embedding 建立完成並已儲存\")\n",
    "\n",
    "\n",
    "\n",
    "# 檢查嵌入總筆數\n",
    "print(\"📦 總筆數：\", vectordb._collection.count())\n",
    "# 顯示前幾筆嵌入資料內容（包括原始文本與 metadata）\n",
    "peek_data = vectordb._collection.get(limit=2)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RU LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ERNO Errors with Traceback ===\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_ru_log_info(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"NO RU log (file not found)\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"NO RU log (error reading file: {e})\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    # Step 1: 優先找 ERNO 與其後續錯誤描述\n",
    "    erno_pattern = re.compile(r\"ERNO 0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\")\n",
    "    error_blocks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if erno_pattern.search(lines[i]):\n",
    "            block = [lines[i]]\n",
    "            ts_prefix = lines[i][:17]  # timestamp 開頭\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                if lines[i].startswith(ts_prefix) or \">>>Oran\" in lines[i] or \" at line \" in lines[i]:\n",
    "                    block.append(lines[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            error_blocks.append(\"\".join(block))\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if error_blocks:\n",
    "        print(\"=== ERNO Errors with Traceback ===\")\n",
    "        return \"\\n\".join(error_blocks)\n",
    "\n",
    "    # Step 2: 擷取 RX-WINDOW-STATS + RX-WINDOW-TIMING 整段\n",
    "    stats_started = False\n",
    "    timing_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"RX-WINDOW-STATS\" in line:\n",
    "            stats_started = True\n",
    "        if stats_started:\n",
    "            timing_block.append(line)\n",
    "            if \"RX_LATEST_C_DL\" in line:\n",
    "                break  # 到 timing 最後一項為止\n",
    "\n",
    "    print(\"=== Timing Window Extracted ===\")\n",
    "    return \"\".join(timing_block)\n",
    "\n",
    "log_result = extract_ru_log_info(ru_log_file)\n",
    "\n",
    "query = log_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (pkt_data, pkt_metadata) \u001b[38;5;129;01min\u001b[39;00m \u001b[43mRawPcapReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpcap_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     38\u001b[0m     packet_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     40\u001b[0m c_plane_count \u001b[38;5;241m=\u001b[39m count_plane_packets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC-Plane\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1385\u001b[0m, in \u001b[0;36mPcapReader_metaclass.__call__\u001b[0;34m(cls, filename)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a cls instance, use the `alternative` if that\u001b[39;00m\n\u001b[1;32m   1376\u001b[0m \u001b[38;5;124;03mfails.\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   1381\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[1;32m   1382\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__bases__\u001b[39m,\n\u001b[1;32m   1383\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[1;32m   1384\u001b[0m )\n\u001b[0;32m-> 1385\u001b[0m filename, fdesc, magic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m magic:\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Scapy_Exception(\n\u001b[1;32m   1388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data could be read!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1389\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scapy/utils.py:1419\u001b[0m, in \u001b[0;36mPcapReader_metaclass.open\u001b[0;34m(fname)\u001b[0m\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1418\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fname\n\u001b[0;32m-> 1419\u001b[0m     fdesc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: _ByteStream\u001b[39;00m\n\u001b[1;32m   1420\u001b[0m     magic \u001b[38;5;241m=\u001b[39m fdesc\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   1421\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\x1f\u001b[39;00m\u001b[38;5;130;01m\\x8b\u001b[39;00m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1422\u001b[0m         \u001b[38;5;66;03m# GZIP header detected.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/aiml/johnson/Scenario/Scenario_3/FH/fh.pcap'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "\n",
    "# pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/normal.pcap\"\n",
    "\n",
    "packet_count = 0\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "def get_packet_count(pcap_file):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_file, \"-q\", \"-z\", \"io,stat,0\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    for line in result.stdout.splitlines():\n",
    "        if \"|   \" in line and \"Frames\" in line:\n",
    "            try:\n",
    "                fields = line.split(\"|\")\n",
    "                frame_info = fields[2].strip()  # Ex: \"X frames\"\n",
    "                frame_count = int(frame_info.split()[0])\n",
    "                return frame_count\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "    packet_count += 1\n",
    "\n",
    "c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "print(\"Control_Plane_Packets: \", c_plane_count)\n",
    "print(\"User_Plane_Packets: \", u_plane_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DU log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found matching log for stage [fh_setup]: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19\n",
      "✅ Found matching log for stage [fh_setup]: [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11)\n",
      "✅ Found matching log for stage [fh_setup]: [PHY]   Detected double sync message 371.6 => 371.7\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"去除ANSI控制字元 + 移除引號 + 去除多餘空格\"\"\"\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    s = ansi_escape.sub('', s)\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "if not Path(du_log_file).exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {du_log_file}\")\n",
    "if not Path(debug_yaml_path).exists():\n",
    "    raise FileNotFoundError(f\"Debug YAML not found: {debug_yaml_path}\")\n",
    "\n",
    "# 讀取 debug.yaml\n",
    "with open(debug_yaml_path, 'r', encoding='utf-8') as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 整理出 (log_snippet, stage) 對應\n",
    "target_entries = []\n",
    "for item in debug_data:\n",
    "    if 'log_snippet' in item:\n",
    "        snippet = item['log_snippet']\n",
    "        stage = item.get('stage', 'unknown')\n",
    "        if \";\" in snippet:\n",
    "            parts = [s.strip() for s in snippet.split(\";\")]\n",
    "            for p in parts:\n",
    "                target_entries.append((p, stage))\n",
    "        else:\n",
    "            target_entries.append((snippet.strip(), stage))\n",
    "\n",
    "# 搜索 log\n",
    "found_results = []\n",
    "with open(du_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for raw_line in f:\n",
    "        line = clean_text(raw_line)\n",
    "        for snippet, stage in target_entries:\n",
    "            snippet_cleaned = clean_text(snippet)\n",
    "            # 🔥 只要關鍵字部分包含就算符合\n",
    "            if snippet_cleaned in line:\n",
    "                found_results.append((stage, snippet))\n",
    "                \n",
    "# 輸出結果\n",
    "if found_results:\n",
    "    for stage, snippet in found_results:\n",
    "        print(f\"✅ Found matching log for stage [{stage}]: {snippet}\")\n",
    "        query = snippet\n",
    "else:\n",
    "    print(\"❌ No matching logs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "- Related config: iq_width_prach, iq_width\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: fh_setup\n",
      "Symptom: DU reports continuous 'Received Time doesn't correspond to the time we think it is' and 'Jump in frame counter' errors after XRAN start. Observed mismatch between expected and received frame/slot numbers and frequent double sync detection.\n",
      "Log: [PHY]   Jump in frame counter last_frame 86 => 167, slot 19; [PHY]   Received Time doesn't correspond to the time we think it is (slot mismatch, received 167.19, expected 86.11); [PHY]   Detected double sync message 371.6 => 371.7\n",
      "- Related config: iq_width_prach, iq_width\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_config_json_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "# RAG prompt_template\n",
    "# prompt_template = f\"\"\"\n",
    "# You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "# Issue Description:\n",
    "# \"{query}\"\n",
    "\n",
    "# Matching debug knowledge:\n",
    "# {matched_case.page_content}\n",
    "# Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "# Reference Device Address Table (external reference file):\n",
    "# {reference_context}\n",
    "\n",
    "# Current configuration block:\n",
    "# {config_segments_context}\n",
    "\n",
    "# Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "# Return a list of JSON objects with the following structure:\n",
    "# [\n",
    "#   {{\n",
    "#     \"label\": \"parameter_name\",\n",
    "#     \"content\": \"parameter_name = (...);\",\n",
    "#     \"reference_reason\": \"Short explanation matching the value to the reference device table (e.g., correct MAC, matches expected setting).\",\n",
    "#     \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
    "#   }},\n",
    "#   ...\n",
    "# ]\n",
    "\n",
    "# - Only include parameters listed in 'Relevant parameters'.\n",
    "# - Do not include any explanation outside of the JSON structure.\n",
    "# - Keep \"reference_reason\" based on the reference table.\n",
    "# - Derive \"model_reason\" from your own technical reasoning.\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration to resolve the described issue based on your technical expertise.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"model_reason\": \"Technical explanation in 1-2 sentences explaining why this change is necessary, beneficial, or resolves the network issue.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "- Only revise parameters that are necessary to resolve the issue.\n",
    "- If no changes are needed, return an empty list: []\n",
    "- Strictly output only valid JSON without any additional text or explanation.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.1-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0,\n",
    "#   top_p=0.7,\n",
    "#   max_tokens=1024,\n",
    "# )\n",
    "\n",
    "# for chunk in client.stream([{\"role\":\"user\",\"content\":\"\"}]): \n",
    "#   print(chunk.content, end=\"\")\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])\n",
    "# print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisions：\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"prach_msg1_FDM\",\n",
      "    \"content\": \"prach_msg1_FDM                                            = 1;\",\n",
      "    \"model_reason\": \"The double sync message suggests collisions in PRACH. Increasing prach_msg1_FDM to 1 (two occasions) reduces contention and the probability of preamble collisions, which can lead to these duplicate sync messages.\"\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"ssb_perRACH_OccasionAndCB_PreamblesPerSSB_PR\",\n",
      "    \"content\": \"ssb_perRACH_OccasionAndCB_PreamblesPerSSB_PR                = 4;\",\n",
      "    \"model_reason\": \"Increasing the number of RACH occasions per SSB also spreads out the random access attempts, thus reducing the likelihood of preamble collisions and the observed double sync messages.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt_template)                                # Gemini API\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisions：\\n\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])            # NIV\n",
    "# # LLM Suggested Revisions\n",
    "# print(\"LLM Suggested Revisions：\\n\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"將 LLM 回傳的 markdown JSON 文字轉成 Python dict\"\"\"\n",
    "    # 移除 markdown 格式包裝\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # 開頭的 ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # 結尾的 ```\n",
    "\n",
    "    # 嘗試解析 JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON 解析失敗:\", e)\n",
    "        print(\"🔍 原始內容:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# ✅ 用法\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # 讀入原始 conf 檔案\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    modified_labels = []\n",
    "\n",
    "    # 依據每個 label 進行替換\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "\n",
    "        # 用正則表達式找原始設定行\n",
    "        pattern = rf\"{label}\\s*=\\s*[^;]+;\"\n",
    "        new_content, count = re.subn(pattern, replacement, content)\n",
    "\n",
    "\n",
    "        if count > 0:\n",
    "            modified_labels.append(label)\n",
    "            content = new_content  # 更新 content 為替換後版本\n",
    "        else:\n",
    "            print(f\"⚠️ No matching setting found ：{label}\")\n",
    "\n",
    "    # 寫入新的 conf 檔案\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"✅ Update file ：{output_path}\")\n",
    "    \n",
    "    # 顯示修改報告\n",
    "    if modified_labels:\n",
    "        print(\"🛠️ Modified parameters：\")\n",
    "        for label in modified_labels:\n",
    "            print(f\" - {label}\")\n",
    "    else:\n",
    "        print(\"📭 No parameters were modified\")\n",
    "\n",
    "# ✅ 執行範例\n",
    "apply_llm_suggestions(\n",
    "    conf_path  =current_config_path,\n",
    "    output_path=none_rag_after_conf_path, \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=current_config_json_path,\n",
    "    output_path=none_rag_after_json_path,\n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
