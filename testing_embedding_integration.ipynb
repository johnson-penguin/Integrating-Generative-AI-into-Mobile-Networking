{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "\n",
    "# 指定 log 檔案\n",
    "du_log_file = \"/home/aiml/johnson/Scenario/Scenario_4/DU/log/Scenario_4.log\"\n",
    "ru_log_file = \"/home/aiml/johnson/Scenario/Scenario_4/RU/log/RU.log\"\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/Scenario/Scenario_4/FH/fh.pcap\"\n",
    "debug_yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "reference_context_path = \"/home/aiml/johnson/Scenario/Scenario_4/reference_config.txt\"\n",
    "\n",
    "current_config_path=\"/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4.conf\"\n",
    "current_config_json_path=\"/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4.conf.segments.json\"\n",
    "\n",
    "after_conf_path=\"/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4_modification_1.conf\"\n",
    "after_json_path=\"/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4_modification_1.conf.segments.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open( debug_yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 將每一筆資料嵌入的格式（以 symptom + log 為主）\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\\n\"\n",
    "\n",
    "    if \"notes\" in item and item[\"notes\"]:\n",
    "        content += f\"Notes: {item['notes']}\\n\"\n",
    "\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # ✅ Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3170939/3660683362.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "2025-04-29 23:30:26.592231: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-29 23:30:26.611146: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-29 23:30:26.611167: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-29 23:30:26.611702: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-29 23:30:26.615126: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-29 23:30:27.002344: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Debug embedding 建立完成並已儲存\n",
      "📦 總筆數： 4\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 115b1faf-ec09-4fa2-a03c-09305fcc9807\n",
      "Document Text: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea4'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown ciphering algorithm 'nea4' in section 'security' of the configuration file\n",
      "Notes: Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "\n",
      "\n",
      "Metadata: {'related_config': 'ciphering_algorithm', 'stage': 'gnb_initialization'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: 5b530a8f-c1af-47fb-9ed5-f7e91fdcf151\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured – possible mismatch in MAC address between DU and RU\n",
      "Notes: ['Ensure that the DU and RU MAC addresses are correctly configured and match each other to enable successful packet capture.']\n",
      "\n",
      "Metadata: {'related_config': 'ru_addr, du_addr', 'stage': 'fh_setup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3170939/3660683362.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# 你也可以改用 Gemini 或 OpenAI embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 將文本嵌入向量並存入 Chroma 資料庫\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"✅ Debug embedding 建立完成並已儲存\")\n",
    "\n",
    "\n",
    "\n",
    "# 檢查嵌入總筆數\n",
    "print(\"📦 總筆數：\", vectordb._collection.count())\n",
    "# 顯示前幾筆嵌入資料內容（包括原始文本與 metadata）\n",
    "peek_data = vectordb._collection.get(limit=2)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check RU LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def extract_ru_log_info(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"NO RU log (file not found)\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"NO RU log (error reading file: {e})\")\n",
    "        return \"NO RU log\"\n",
    "\n",
    "    # Step 1: 優先找 ERNO 與其後續錯誤描述\n",
    "    erno_pattern = re.compile(r\"ERNO 0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\\s+0x[0-9A-Fa-f]+\")\n",
    "    error_blocks = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        if erno_pattern.search(lines[i]):\n",
    "            block = [lines[i]]\n",
    "            ts_prefix = lines[i][:17]  # timestamp 開頭\n",
    "            i += 1\n",
    "            while i < len(lines):\n",
    "                if lines[i].startswith(ts_prefix) or \">>>Oran\" in lines[i] or \" at line \" in lines[i]:\n",
    "                    block.append(lines[i])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    break\n",
    "            error_blocks.append(\"\".join(block))\n",
    "        else:\n",
    "            i += 1\n",
    "\n",
    "    if error_blocks:\n",
    "        print(\"=== ERNO Errors with Traceback ===\")\n",
    "        return \"\\n\".join(error_blocks)\n",
    "\n",
    "    # Step 2: 擷取 RX-WINDOW-STATS + RX-WINDOW-TIMING 整段\n",
    "    stats_started = False\n",
    "    timing_block = []\n",
    "\n",
    "    for line in lines:\n",
    "        if \"RX-WINDOW-STATS\" in line:\n",
    "            stats_started = True\n",
    "        if stats_started:\n",
    "            timing_block.append(line)\n",
    "            if \"RX_LATEST_C_DL\" in line:\n",
    "                break  # 到 timing 最後一項為止\n",
    "\n",
    "    print(\"=== Timing Window Extracted ===\")\n",
    "    return \"\".join(timing_block)\n",
    "\n",
    "log_result = extract_ru_log_info(ru_log_file)\n",
    "\n",
    "query = log_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check FH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "\n",
    "# pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/normal.pcap\"\n",
    "\n",
    "packet_count = 0\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "def get_packet_count(pcap_file):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_file, \"-q\", \"-z\", \"io,stat,0\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    for line in result.stdout.splitlines():\n",
    "        if \"|   \" in line and \"Frames\" in line:\n",
    "            try:\n",
    "                fields = line.split(\"|\")\n",
    "                frame_info = fields[2].strip()  # Ex: \"X frames\"\n",
    "                frame_count = int(frame_info.split()[0])\n",
    "                return frame_count\n",
    "            except Exception:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "    packet_count += 1\n",
    "\n",
    "c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "print(\"Control_Plane_Packets: \", c_plane_count)\n",
    "print(\"User_Plane_Packets: \", u_plane_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check DU log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Found matching log for stage [gnb_initialization]: [RRC]   unknown ciphering algorithm 'nea4' in section 'security' of the configuration file\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def clean_text(s):\n",
    "    \"\"\"去除ANSI控制字元 + 移除引號 + 去除多餘空格\"\"\"\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    s = ansi_escape.sub('', s)\n",
    "    s = s.replace(\"'\", \"\").replace('\"', \"\")\n",
    "    s = s.strip()\n",
    "    return s\n",
    "\n",
    "if not Path(du_log_file).exists():\n",
    "    raise FileNotFoundError(f\"Log file not found: {du_log_file}\")\n",
    "if not Path(debug_yaml_path).exists():\n",
    "    raise FileNotFoundError(f\"Debug YAML not found: {debug_yaml_path}\")\n",
    "\n",
    "# 讀取 debug.yaml\n",
    "with open(debug_yaml_path, 'r', encoding='utf-8') as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# 整理出 (log_snippet, stage) 對應\n",
    "target_entries = []\n",
    "for item in debug_data:\n",
    "    if 'log_snippet' in item:\n",
    "        snippet = item['log_snippet']\n",
    "        stage = item.get('stage', 'unknown')\n",
    "        if \";\" in snippet:\n",
    "            parts = [s.strip() for s in snippet.split(\";\")]\n",
    "            for p in parts:\n",
    "                target_entries.append((p, stage))\n",
    "        else:\n",
    "            target_entries.append((snippet.strip(), stage))\n",
    "\n",
    "# 搜索 log\n",
    "found_results = []\n",
    "with open(du_log_file, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for raw_line in f:\n",
    "        line = clean_text(raw_line)\n",
    "        for snippet, stage in target_entries:\n",
    "            snippet_cleaned = clean_text(snippet)\n",
    "            # 🔥 只要關鍵字部分包含就算符合\n",
    "            if snippet_cleaned in line:\n",
    "                found_results.append((stage, snippet))\n",
    "                \n",
    "# 輸出結果\n",
    "if found_results:\n",
    "    for stage, snippet in found_results:\n",
    "        print(f\"✅ Found matching log for stage [{stage}]: {snippet}\")\n",
    "        query = snippet\n",
    "else:\n",
    "    print(\"❌ No matching logs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: gnb_initialization\n",
      "Symptom: gNB failed to recognize the configured ciphering algorithm ('nea4'), resulting in security module initialization error and potential connection failure with UE.\n",
      "Log: [RRC]   unknown ciphering algorithm 'nea4' in section 'security' of the configuration file\n",
      "Notes: Supported ciphering algorithms:\n",
      "  - nea0 (no encryption)\n",
      "  - nea1 (based on SNOW 3G)\n",
      "  - nea2 (based on AES)\n",
      "  - nea3 (based on ZUC)\n",
      "It is recommended to use 'nea0' (no encryption) first to simplify initial debugging and avoid security negotiation failures.\n",
      "\n",
      "\n",
      "- Related config: ciphering_algorithm\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured – possible mismatch in MAC address between DU and RU\n",
      "Notes: ['Ensure that the DU and RU MAC addresses are correctly configured and match each other to enable successful packet capture.']\n",
      "\n",
      "- Related config: ru_addr, du_addr\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(current_config_json_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Matching debug knowledge:\n",
    "{matched_case.page_content}\n",
    "Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "Reference Device Address Table (external reference file):\n",
    "{reference_context}\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"reference_reason\": \"Short explanation matching the value to the reference device table (e.g., correct MAC, matches expected setting).\",\n",
    "    \"model_reason\": \"Additional expert analysis in 1-2 sentences explaining why this change is necessary, beneficial, or resolves a network issue.\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "- Only include parameters listed in 'Relevant parameters'.\n",
    "- Do not include any explanation outside of the JSON structure.\n",
    "- Keep \"reference_reason\" based on the reference table.\n",
    "- Derive \"model_reason\" from your own technical reasoning.\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM API 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "# for m in genai.list_models():\n",
    "#     print(m.name)\n",
    "\n",
    "####################################################################################################################################################################################\n",
    "\n",
    "# from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "# client = ChatNVIDIA(\n",
    "#   model=\"meta/llama-3.1-70b-instruct\",\n",
    "#   api_key=\"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\", \n",
    "#   temperature=0,\n",
    "#   top_p=0.7,\n",
    "#   max_tokens=1024,\n",
    "# )\n",
    "\n",
    "# for chunk in client.stream([{\"role\":\"user\",\"content\":\"\"}]): \n",
    "#   print(chunk.content, end=\"\")\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])\n",
    "# print(response.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisions：\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"ciphering_algorithms\",\n",
      "    \"content\": \"ciphering_algorithms = ( \\\"nea0\\\" );\",\n",
      "    \"reference_reason\": \"Recommended to use 'nea0' (no encryption) for initial debugging.\",\n",
      "    \"model_reason\": \"Changing ciphering algorithm to nea0 disables encryption, simplifying the initial setup. This avoids potential security negotiation failures during gNB initialization, as the error log indicates an unknown ciphering algorithm.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = model.generate_content(prompt_template)                                # Gemini API\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisions：\\n\")\n",
    "print(response.text)\n",
    "\n",
    "\n",
    "\n",
    "# response = client.invoke([{\"role\": \"user\", \"content\": prompt_template}])            # NIV\n",
    "# # LLM Suggested Revisions\n",
    "# print(\"LLM Suggested Revisions：\\n\")\n",
    "# print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ciphering_algorithms', 'content': 'ciphering_algorithms = ( \"nea0\" );'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"將 LLM 回傳的 markdown JSON 文字轉成 Python dict\"\"\"\n",
    "    # 移除 markdown 格式包裝\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # 開頭的 ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # 結尾的 ```\n",
    "\n",
    "    # 嘗試解析 JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON 解析失敗:\", e)\n",
    "        print(\"🔍 原始內容:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# ✅ 用法\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4_modification_1.conf\n",
      "🛠️ Modified parameters：\n",
      " - ciphering_algorithms\n",
      "✅ Update file ：/home/aiml/johnson/Scenario/Scenario_4/DU/conf/Scenario_4_modification_1.conf.segments.json\n",
      "🛠️ Modified parameters：\n",
      " - ciphering_algorithms\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # 讀入原始 conf 檔案\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    modified_labels = []\n",
    "\n",
    "    # 依據每個 label 進行替換\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "\n",
    "        # 用正則表達式找原始設定行\n",
    "        pattern = rf\"{label}\\s*=\\s*[^;]+;\"\n",
    "        new_content, count = re.subn(pattern, replacement, content)\n",
    "\n",
    "\n",
    "        if count > 0:\n",
    "            modified_labels.append(label)\n",
    "            content = new_content  # 更新 content 為替換後版本\n",
    "        else:\n",
    "            print(f\"⚠️ No matching setting found ：{label}\")\n",
    "\n",
    "    # 寫入新的 conf 檔案\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"✅ Update file ：{output_path}\")\n",
    "    \n",
    "    # 顯示修改報告\n",
    "    if modified_labels:\n",
    "        print(\"🛠️ Modified parameters：\")\n",
    "        for label in modified_labels:\n",
    "            print(f\" - {label}\")\n",
    "    else:\n",
    "        print(\"📭 No parameters were modified\")\n",
    "\n",
    "# ✅ 執行範例\n",
    "apply_llm_suggestions(\n",
    "    conf_path  =current_config_path,\n",
    "    output_path=after_conf_path, \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=current_config_json_path,\n",
    "    output_path=after_json_path,\n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
