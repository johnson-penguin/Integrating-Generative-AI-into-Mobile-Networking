{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/gemini-2.5-pro-exp-03-25\n",
      "models/gemini-2.5-pro-preview-03-25\n",
      "models/gemini-2.5-flash-preview-04-17\n",
      "models/gemini-2.0-flash-exp\n",
      "models/gemini-2.0-flash\n",
      "models/gemini-2.0-flash-001\n",
      "models/gemini-2.0-flash-exp-image-generation\n",
      "models/gemini-2.0-flash-lite-001\n",
      "models/gemini-2.0-flash-lite\n",
      "models/gemini-2.0-flash-lite-preview-02-05\n",
      "models/gemini-2.0-flash-lite-preview\n",
      "models/gemini-2.0-pro-exp\n",
      "models/gemini-2.0-pro-exp-02-05\n",
      "models/gemini-exp-1206\n",
      "models/gemini-2.0-flash-thinking-exp-01-21\n",
      "models/gemini-2.0-flash-thinking-exp\n",
      "models/gemini-2.0-flash-thinking-exp-1219\n",
      "models/learnlm-1.5-pro-experimental\n",
      "models/learnlm-2.0-flash-experimental\n",
      "models/gemma-3-1b-it\n",
      "models/gemma-3-4b-it\n",
      "models/gemma-3-12b-it\n",
      "models/gemma-3-27b-it\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n",
      "models/aqa\n",
      "models/imagen-3.0-generate-002\n",
      "models/gemini-2.0-flash-live-001\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import json\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "import pprint\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=\"AIzaSyCSK7WFIon0kt_iPbqvzaJqwI9vNE5mwdM\")\n",
    "for m in genai.list_models():\n",
    "    print(m.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_path = \"/home/aiml/johnson/thesis_rag/Integration_dataset/debug.yaml\"\n",
    "with open( yaml_path , \"r\") as f:\n",
    "    debug_data = yaml.safe_load(f)\n",
    "\n",
    "# å°‡æ¯ä¸€ç­†è³‡æ–™åµŒå…¥çš„æ ¼å¼ï¼ˆä»¥ symptom + log ç‚ºä¸»ï¼‰\n",
    "embedding_docs = []\n",
    "for item in debug_data:\n",
    "    content = f\"Stage: {item['stage']}\\nSymptom: {item['symptom']}\\nLog: {item['log_snippet']}\"\n",
    "    related_config_str = \", \".join(item[\"related_config\"])  # âœ… Convert list to comma-separated string\n",
    "    metadata = {\n",
    "        \"stage\": item[\"stage\"],\n",
    "        \"related_config\": related_config_str\n",
    "    }\n",
    "    embedding_docs.append({\"content\": content, \"metadata\": metadata})\n",
    "\n",
    "# pprint.pprint(embedding_docs) #for checking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1104271/3660683362.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
      "2025-04-24 17:13:08.246661: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-24 17:13:08.265546: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-04-24 17:13:08.265569: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-04-24 17:13:08.266097: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-24 17:13:08.269381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-24 17:13:08.677719: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Debug embedding å»ºç«‹å®Œæˆä¸¦å·²å„²å­˜\n",
      "ğŸ“¦ ç¸½ç­†æ•¸ï¼š 24\n",
      "\n",
      "--- Entry 1 ---\n",
      "Document ID: 9cb88e8e-6d25-4253-bcfd-5a6885a8c13b\n",
      "Document Text: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured â€“ possible mismatch in MAC address between DU and RU\n",
      "Metadata: {'related_config': 'ru_addr, du_addr', 'stage': 'fh_setup'}\n",
      "\n",
      "--- Entry 2 ---\n",
      "Document ID: f2370200-85f2-4257-a257-5bc500d10a6d\n",
      "Document Text: Stage: ng_setup\n",
      "Symptom: Incorrect AMF IP configuration, unable to establish NG connection\n",
      "Log: [SCTP] Connect failed: Connection refused\n",
      "Metadata: {'related_config': 'amf_ip_address', 'stage': 'ng_setup'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1104271/3660683362.py:9: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "# ä½ ä¹Ÿå¯ä»¥æ”¹ç”¨ Gemini æˆ– OpenAI embedding\n",
    "embedding = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# å°‡æ–‡æœ¬åµŒå…¥å‘é‡ä¸¦å­˜å…¥ Chroma è³‡æ–™åº«\n",
    "texts = [d[\"content\"] for d in embedding_docs]\n",
    "metadatas = [d[\"metadata\"] for d in embedding_docs]\n",
    "\n",
    "vectordb = Chroma.from_texts(texts, embedding=embedding, metadatas=metadatas, persist_directory=\"./error_db\")\n",
    "vectordb.persist()\n",
    "\n",
    "print(\"âœ… Debug embedding å»ºç«‹å®Œæˆä¸¦å·²å„²å­˜\")\n",
    "\n",
    "\n",
    "\n",
    "# æª¢æŸ¥åµŒå…¥ç¸½ç­†æ•¸\n",
    "print(\"ğŸ“¦ ç¸½ç­†æ•¸ï¼š\", vectordb._collection.count())\n",
    "# é¡¯ç¤ºå‰å¹¾ç­†åµŒå…¥è³‡æ–™å…§å®¹ï¼ˆåŒ…æ‹¬åŸå§‹æ–‡æœ¬èˆ‡ metadataï¼‰\n",
    "peek_data = vectordb._collection.get(limit=2)\n",
    "\n",
    "for i in range(len(peek_data[\"documents\"])):\n",
    "    print(f\"\\n--- Entry {i+1} ---\")\n",
    "    print(\"Document ID:\", peek_data[\"ids\"][i])\n",
    "    print(\"Document Text:\", peek_data[\"documents\"][i])\n",
    "    print(\"Metadata:\", peek_data[\"metadatas\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCAP receive no FH Packet\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from scapy.utils import RawPcapReader\n",
    "\n",
    "pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/no_u_plane.pcap\"\n",
    "# pcap_path = \"/home/aiml/johnson/thesis_rag/fh_pcap_sample/normal.pcap\"\n",
    "\n",
    "packet_count = 0\n",
    "\n",
    "# Filter the packet content through tshark and search for the keywords \"C-Plane\" and \"U-Plane\"\n",
    "def count_plane_packets(keyword):\n",
    "    result = subprocess.run(\n",
    "        [\"tshark\", \"-r\", pcap_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    lines = result.stdout.splitlines()\n",
    "    return sum(1 for line in lines if keyword in line)\n",
    "\n",
    "for (pkt_data, pkt_metadata) in RawPcapReader(pcap_path):\n",
    "    packet_count += 1\n",
    "\n",
    "packet_count = 0 # for testing\n",
    "\n",
    "if packet_count != 0:\n",
    "    query = \"PCAP loading successfully, total number of packets:\", packet_count\n",
    "    print(query)\n",
    "else:\n",
    "    query = \"PCAP receive no FH Packet\"\n",
    "    print(query)\n",
    "\n",
    "# c_plane_count = count_plane_packets(\"C-Plane\")\n",
    "# u_plane_count = count_plane_packets(\"U-Plane\")\n",
    "\n",
    "# print(\"Control_Plane_Packets: \", c_plane_count)\n",
    "# print(\"User_Plane_Packets: \", u_plane_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matched: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured â€“ possible mismatch in MAC address between DU and RU\n",
      "- Related config: ru_addr, du_addr\n",
      "-----------------------------------------------\n",
      "- Matched: Stage: fh_setup\n",
      "Symptom: PCAP file contains zero packets, likely due to incorrect MAC address configuration\n",
      "Log: [FH] No packets captured â€“ possible mismatch in MAC address between DU and RU\n",
      "- Related config: ru_addr, du_addr\n",
      "-----------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = vectordb.similarity_search(query, k=2)\n",
    "\n",
    "for r in results:\n",
    "    print(\"- Matched:\", r.page_content)\n",
    "    print(\"- Related config:\", r.metadata[\"related_config\"])\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "matched_case = results[0]\n",
    "matched_symptom = matched_case.page_content\n",
    "matched_related_config = matched_case.metadata.get(\"related_config\", \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_context_path = \"/home/aiml/johnson/thesis_rag/sample/reference_config.txt\"\n",
    "current_config_path = \"/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample.conf.segments.json\"\n",
    "\n",
    "with open(current_config_path, \"r\") as f:\n",
    "    config_segments_context = json.load(f)\n",
    "with open(reference_context_path, \"r\") as f:\n",
    "    reference_context = f.read()\n",
    "\n",
    "prompt_template = f\"\"\"\n",
    "You are a 5G network expert. Your job is to revise configuration files based on observed network issues and debug knowledge.\n",
    "\n",
    "Issue Description:\n",
    "\"{query}\"\n",
    "\n",
    "Matching debug knowledge:\n",
    "{matched_case.page_content}\n",
    "Relevant parameters: {matched_case.metadata[\"related_config\"]}\n",
    "\n",
    "Reference Device Address Table (external reference file):\n",
    "{reference_context}\n",
    "\n",
    "Current configuration block:\n",
    "{config_segments_context}\n",
    "\n",
    "Please revise the configuration using correct addresses from the reference. Output only the revised config section.\n",
    "\n",
    "Return a list of JSON objects with the following structure:\n",
    "[\n",
    "  {{\n",
    "    \"label\": \"parameter_name\",\n",
    "    \"content\": \"parameter_name = (...);\",\n",
    "    \"reason\": \"Short explanation of why this value is used (e.g., matches reference, correct MAC, typical setting, etc.)\"\n",
    "  }},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Only include parameters listed in 'Relevant parameters'. Do not include explanation outside of the JSON structure.\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Suggested Revisionsï¼š\n",
      "\n",
      "```json\n",
      "[\n",
      "  {\n",
      "    \"label\": \"ru_addr\",\n",
      "    \"content\": \"ru_addr = (\\\"10:70:fd:14:1c:10\\\", \\\"10:70:fd:14:1c:10\\\");\",\n",
      "    \"reason\": \"Matches the RU MAC address from the reference device address table.\"\n",
      "  },\n",
      "  {\n",
      "    \"label\": \"du_addr\",\n",
      "    \"content\": \"du_addr = (\\\"00:11:22:33:44:66\\\", \\\"00:11:22:33:44:66\\\");\",\n",
      "    \"reason\": \"Matches the DU MAC address from the reference device address table.\"\n",
      "  }\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\"gemini-2.0-flash\")\n",
    "response = model.generate_content(prompt_template)\n",
    "\n",
    "# LLM Suggested Revisions\n",
    "print(\"LLM Suggested Revisionsï¼š\\n\")\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'ru_addr', 'content': 'ru_addr = (\"10:70:fd:14:1c:10\", \"10:70:fd:14:1c:10\");'}, {'label': 'du_addr', 'content': 'du_addr = (\"00:11:22:33:44:66\", \"00:11:22:33:44:66\");'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_llm_response(text):\n",
    "    \"\"\"å°‡ LLM å›å‚³çš„ markdown JSON æ–‡å­—è½‰æˆ Python dict\"\"\"\n",
    "    # ç§»é™¤ markdown æ ¼å¼åŒ…è£\n",
    "    cleaned = text.strip()\n",
    "    cleaned = re.sub(r\"^```json\\s*\", \"\", cleaned)   # é–‹é ­çš„ ```json\n",
    "    cleaned = re.sub(r\"\\s*```$\", \"\", cleaned)       # çµå°¾çš„ ```\n",
    "\n",
    "    # å˜—è©¦è§£æ JSON\n",
    "    try:\n",
    "        parsed = json.loads(cleaned)\n",
    "        llm_suggestions = [\n",
    "            {\"label\": item[\"label\"], \"content\": item[\"content\"]}\n",
    "            for item in parsed\n",
    "        ]\n",
    "        return llm_suggestions\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"âŒ JSON è§£æå¤±æ•—:\", e)\n",
    "        print(\"ğŸ” åŸå§‹å…§å®¹:\\n\", cleaned)\n",
    "        return []\n",
    "\n",
    "# âœ… ç”¨æ³•\n",
    "llm_suggestions = parse_llm_response(response.text)\n",
    "print(llm_suggestions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²å¯«å…¥æ›´æ–°æª”æ¡ˆï¼š/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample_1.conf\n",
      "âœ… å·²å¯«å…¥æ›´æ–°æª”æ¡ˆï¼š/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample.conf.segments_1.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def apply_llm_suggestions(conf_path, output_path, llm_suggestions):\n",
    "    # è®€å…¥åŸå§‹ conf æª”æ¡ˆ\n",
    "    with open(conf_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "\n",
    "    # ä¾æ“šæ¯å€‹ label é€²è¡Œæ›¿æ›\n",
    "    for suggestion in llm_suggestions:\n",
    "        label = suggestion[\"label\"]\n",
    "        replacement = suggestion[\"content\"]\n",
    "        # ç”¨æ­£å‰‡è¡¨é”å¼æŠ“å‡ºå°æ‡‰çš„è¨­å®šè¡Œï¼Œä¸¦æ›¿æ›\n",
    "        pattern = rf\"{label}\\s*=\\s*\\(.*?\\);\"\n",
    "        content = re.sub(pattern, replacement, content)\n",
    "\n",
    "    # å¯«å…¥æ–°çš„ conf æª”æ¡ˆ\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "    print(f\"âœ… å·²å¯«å…¥æ›´æ–°æª”æ¡ˆï¼š{output_path}\")\n",
    "\n",
    "# âœ… åŸ·è¡Œç¯„ä¾‹\n",
    "apply_llm_suggestions(\n",
    "    conf_path=\"/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample.conf\", \n",
    "    output_path=\"/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample_1.conf\", \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n",
    "\n",
    "apply_llm_suggestions(\n",
    "    conf_path=\"/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample.conf.segments.json\", \n",
    "    output_path=\"/home/aiml/johnson/thesis_rag/Conf_to_Json_part/sample.conf.segments_1.json\", \n",
    "    llm_suggestions=llm_suggestions\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
